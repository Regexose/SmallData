{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>utterance</th>\n",
       "      <th>category</th>\n",
       "      <th>category_id</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Merkel war und ist hier eine getriebene</td>\n",
       "      <td>insinuation</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Das ist doch albern.</td>\n",
       "      <td>lecture</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dass ich nicht lache</td>\n",
       "      <td>dissence</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sie haben schlicht gar nichts begriffen.</td>\n",
       "      <td>lecture</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Stellen Sie sich nicht dumm</td>\n",
       "      <td>lecture</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  utterance     category  category_id  \\\n",
       "0   Merkel war und ist hier eine getriebene  insinuation            0   \n",
       "1                      Das ist doch albern.      lecture            1   \n",
       "2                      Dass ich nicht lache     dissence            2   \n",
       "3  Sie haben schlicht gar nichts begriffen.      lecture            1   \n",
       "4               Stellen Sie sich nicht dumm      lecture            1   \n",
       "\n",
       "   word_count  \n",
       "0           7  \n",
       "1           4  \n",
       "2           4  \n",
       "3           6  \n",
       "4           5  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# taken from https://towardsdatascience.com/multi-class-text-classification-with-scikit-learn-12f1e60e0a9f\n",
    "\n",
    "# Load data and prepare data-frame\n",
    "# - strip unused columns \n",
    "# - add additional information\n",
    "# - remove too short utterances\n",
    "\n",
    "import pandas as pd\n",
    "min_word_count = 1\n",
    "categories_to_consider = ['praise', 'dissence', 'lecture', 'concession', 'insinuation']\n",
    "\n",
    "file = '../webserver/model_data/TrainingData_ml.xlsx'\n",
    "col = ['utterance', 'category']\n",
    "df = pd.read_excel(file).rename(columns={'Effekt': 'category'})\n",
    "\n",
    "df = df[col]\n",
    "df = df[pd.notnull(df['utterance'])]\n",
    "df.columns = ['utterance', 'category']\n",
    "\n",
    "df = df[df['category'].isin(categories_to_consider)]         \n",
    "\n",
    "\n",
    "df['category_id'] = df['category'].factorize()[0]\n",
    "df['word_count'] = list(map(lambda x:len(x.split(' ')), df['utterance']))\n",
    "category_id_df = df[['category', 'category_id']].drop_duplicates().sort_values('category_id')\n",
    "category_to_id = dict(category_id_df.values)\n",
    "id_to_category = dict(category_id_df[['category_id', 'category']].values)\n",
    "# filter for minimal word count\n",
    "df = df[df['word_count']>=min_word_count]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Analyze category distribution\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure(figsize=(8,6))\n",
    "df.groupby('category').utterance.count().plot.bar(ylim=0)\n",
    "plt.title('Training distribution, n={}'.format(len(df)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'stop_words'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-77f1dff8f682>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mstop_words\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_stop_words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mvectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'german_model'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'stop_words'"
     ]
    }
   ],
   "source": [
    "# Vectorization\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from stop_words import get_stop_words\n",
    "\n",
    "vectorizer = 'german_model'\n",
    "if vectorizer == 'tfifd':\n",
    "    tfidf = TfidfVectorizer(sublinear_tf=True, min_df=2, norm='l2', encoding='latin-1', ngram_range=(1, 2))\n",
    "    # , stop_words=get_stop_words('de')\n",
    "    features = tfidf.fit_transform(df.utterance).toarray()\n",
    "    labels = df.category_id\n",
    "elif vectorizer == 'german_model':\n",
    "    import sys, os\n",
    "    sys.path.insert(0, os.path.abspath('..'))\n",
    "    from webserver.classification import trainer\n",
    "    sentences, categories = trainer.read_trainingdata_utterances(df)\n",
    "    features, labels = trainer.vectorize_corpus(sentences, trainer.sentence_to_vec_german_model, categories=categories)\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze predictive features\n",
    "\n",
    "from sklearn.feature_selection import chi2\n",
    "import numpy as np\n",
    "N = 2\n",
    "features_chi2 = {}\n",
    "feature_names = {}\n",
    "\n",
    "for Effekt, category_id in sorted(category_to_id.items()):\n",
    "  raw_chi2 = chi2(features, labels == category_id)\n",
    "  indices = np.argsort(raw_chi2[0])[::-1]\n",
    "  feature_names[category_id] = np.array(tfidf.get_feature_names())[indices]\n",
    "  features_chi2[category_id] = raw_chi2[0][indices]\n",
    "  unigrams = [v for v in feature_names[category_id] if len(v.split(' ')) == 1]\n",
    "  bigrams = [v for v in feature_names[category_id] if len(v.split(' ')) == 2]\n",
    "  print(\"# '{}':\".format(Effekt))\n",
    "  print(\"  . Most correlated unigrams:\\n. {}\".format('\\n. '.join(unigrams[0:N])))\n",
    "  print(\"  . Most correlated bigrams:\\n. {}\".format('\\n. '.join(bigrams[0:N])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 20\n",
    "for index, cat in id_to_category.items():\n",
    "    fig = plt.figure(figsize=[20, 6])\n",
    "    plt.bar(range(N), features_chi2[index][0:N])\n",
    "    plt.xticks(range(N), feature_names[index][0:N])\n",
    "    plt.title(cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['utterance'], df['category'], random_state = 0)\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(X_train)\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "clf = MultinomialNB().fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(clf.predict(count_vect.transform([\"Da musst du aber noch mal ran, das muss noch besser werden!\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "models = [\n",
    "    RandomForestClassifier(n_estimators=200, max_depth=3, random_state=0),\n",
    "    LinearSVC(),\n",
    "    SGDClassifier(tol=1e-3, max_iter=1000, loss='modified_huber'),\n",
    "    MultinomialNB(),\n",
    "    LogisticRegression(random_state=0),\n",
    "]\n",
    "\n",
    "CV = 5\n",
    "cv_df = pd.DataFrame(index=range(CV * len(models)))\n",
    "entries = []\n",
    "for model in models:\n",
    "  model_name = model.__class__.__name__\n",
    "  accuracies = cross_val_score(model, features, labels, scoring='accuracy', cv=CV)\n",
    "  for fold_idx, accuracy in enumerate(accuracies):\n",
    "    entries.append((model_name, fold_idx, accuracy))\n",
    "cv_df = pd.DataFrame(entries, columns=['model_name', 'fold_idx', 'accuracy'])\n",
    "import seaborn as sns\n",
    "sns.boxplot(x='model_name', y='accuracy', data=cv_df)\n",
    "sns.stripplot(x='model_name', y='accuracy', data=cv_df, \n",
    "              size=8, jitter=True, edgecolor=\"gray\", linewidth=2)\n",
    "plt.show()\n",
    "cv_df.groupby('model_name').accuracy.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "model = SGDClassifier(tol=1e-3, max_iter=1000, random_state=0, loss='modified_huber')\n",
    "model = MultinomialNB()\n",
    "X_train, X_test, y_train, y_test, indices_train, indices_test = train_test_split(features, labels, range(len(labels)), test_size=0.33, random_state=0)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "conf_mat = confusion_matrix(y_test, y_pred)\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "sns.heatmap(conf_mat, annot=True, fmt='d',\n",
    "            xticklabels=category_id_df.category.values, yticklabels=category_id_df.category.values)\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "for predicted in category_id_df.category_id:\n",
    "  for actual in category_id_df.category_id:\n",
    "    if predicted != actual and conf_mat[actual, predicted] >= 10:\n",
    "      print(\"'{}' predicted as '{}' : {} examples.\".format(id_to_category[actual], id_to_category[predicted], conf_mat[actual, predicted]))\n",
    "      display(df.loc[indices_test[(y_test == actual) & (y_pred == predicted)]][['category', 'utterance']])\n",
    "      print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check doc2vec\n",
    "# taken from https://medium.com/towards-artificial-intelligence/text-classification-by-xgboost-others-a-case-study-using-bbc-news-articles-5d88e94a9f8\n",
    "import pandas as pd\n",
    "\n",
    "file = '../webserver/model_data/TrainingData_ml.xlsx'\n",
    "df = pd.read_excel(file).rename(columns={'Effekt': 'category'})\n",
    "col = ['utterance', 'category']\n",
    "df = df[col]\n",
    "\n",
    "from gensim import utils\n",
    "import gensim.parsing.preprocessing as gsp\n",
    "\n",
    "filters = [\n",
    "           gsp.strip_tags, \n",
    "           gsp.strip_punctuation,\n",
    "           gsp.strip_multiple_whitespaces,\n",
    "           gsp.strip_numeric,\n",
    "           gsp.strip_short, \n",
    "          ]\n",
    "\n",
    "def clean_text(s):\n",
    "    s = s.lower()\n",
    "    s = utils.to_unicode(s)\n",
    "    for f in filters:\n",
    "        s = f(s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "\n",
    "def plot_word_cloud(text):\n",
    "    wordcloud_instance = WordCloud(width = 800, height = 800, \n",
    "                background_color ='black', \n",
    "                stopwords=None,\n",
    "                min_font_size = 10).generate(text) \n",
    "             \n",
    "    plt.figure(figsize = (8, 8), facecolor = None) \n",
    "    plt.imshow(wordcloud_instance) \n",
    "    plt.axis(\"off\") \n",
    "    plt.tight_layout(pad = 0) \n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "texts = ''\n",
    "for index, item in df.iterrows():\n",
    "    texts = texts + ' ' + clean_text(item['utterance'])\n",
    "    \n",
    "plot_word_cloud(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_word_cloud_for_category(df, category):\n",
    "    text_df = df.loc[df['category'] == str(category)]\n",
    "    texts = ''\n",
    "    for index, item in text_df.iterrows():\n",
    "        texts = texts + ' ' + clean_text(item['utterance'])\n",
    "    \n",
    "    plot_word_cloud(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_word_cloud_for_category(df,'praise')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
