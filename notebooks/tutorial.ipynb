{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading /Users/staude/Projekte/BorisJoens/SmallData/webserver/model_data/TrainingDataPelle02.tsv\n",
      "Loading /Users/staude/Projekte/BorisJoens/SmallData/webserver/model_data/TrainingDataPelle01.tsv\n",
      "Loading /Users/staude/Projekte/BorisJoens/SmallData/webserver/model_data/TrainingDataBoris01.tsv\n",
      "Loading /Users/staude/Projekte/BorisJoens/SmallData/webserver/model_data/TrainingDataBoris02.tsv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>utterance</th>\n",
       "      <th>category</th>\n",
       "      <th>word_count</th>\n",
       "      <th>category_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Wer die Weakerthans nennt, darf Propagandhi ni...</td>\n",
       "      <td>lecture</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sorry schwerer Irrtum.</td>\n",
       "      <td>dissence</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Juhu, Petric der Held.</td>\n",
       "      <td>praise</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>das ist ja total ätzend.</td>\n",
       "      <td>dissence</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Allein mit dem Track hat er die ganze Deutschr...</td>\n",
       "      <td>praise</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           utterance  category  word_count  \\\n",
       "1  Wer die Weakerthans nennt, darf Propagandhi ni...   lecture           9   \n",
       "3                             Sorry schwerer Irrtum.  dissence           3   \n",
       "4                             Juhu, Petric der Held.    praise           4   \n",
       "5                           das ist ja total ätzend.  dissence           5   \n",
       "7  Allein mit dem Track hat er die ganze Deutschr...    praise          14   \n",
       "\n",
       "   category_id  \n",
       "1            2  \n",
       "3            1  \n",
       "4            0  \n",
       "5            1  \n",
       "7            0  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# taken from https://towardsdatascience.com/multi-class-text-classification-with-scikit-learn-12f1e60e0a9f\n",
    "\n",
    "# Load data and prepare data-frame\n",
    "# - strip unused columns \n",
    "# - add additional information\n",
    "# - remove too short utterances\n",
    "\n",
    "import pandas as pd\n",
    "import sys\n",
    "from os import path\n",
    "\n",
    "sys.path.append(path.abspath('..'))\n",
    "from webserver.classification import trainer\n",
    "min_word_count = 2\n",
    "\n",
    "\n",
    "data_path = '../webserver/model_data'\n",
    "col = ['utterance', 'category']\n",
    "categories_to_consider = ['praise', 'dissence', 'lecture', 'concession', 'insinuation']\n",
    "\n",
    "td_pattern = 'TrainingData(Boris|Pelle)[0-9][0-9].tsv'\n",
    "df = trainer.load_training_files(data_path, td_pattern, categories_to_consider) \n",
    "\n",
    "category_id_df = df[['category', 'category_id']].drop_duplicates().sort_values('category_id')\n",
    "category_to_id = dict(category_id_df.values)\n",
    "id_to_category = dict(category_id_df[['category_id', 'category']].values)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze category distribution\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure(figsize=(8,6))\n",
    "df.groupby('category').utterance.count().plot.bar(ylim=0)\n",
    "plt.title('Training distribution, n={}'.format(len(df)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorization\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from stop_words import get_stop_words\n",
    "import numpy as np\n",
    "\n",
    "vectorizer_name = 'tfidf'\n",
    "def vectorize(df, vectorizer_name):\n",
    "    if vectorizer_name == 'tfidf':\n",
    "        tfidf = TfidfVectorizer(sublinear_tf=True, min_df=2, norm='l2', encoding='latin-1', ngram_range=(1, 2))\n",
    "        # , stop_words=get_stop_words('de'))\n",
    "        features = tfidf.fit_transform(df.utterance).toarray()\n",
    "    elif vectorizer_name == 'german_model':\n",
    "        import sys, os\n",
    "        sys.path.insert(0, os.path.abspath('..'))\n",
    "        from webserver.classification import trainer\n",
    "        sentences = df.utterance.to_list()\n",
    "        categories = df.category_id.to_list()\n",
    "        sentences_cleaned, features, labels = trainer.vectorize_corpus(sentences, trainer.sentence_to_vec_german_model, categories=categories, representation=trainer.REPRESENTATION_TYPE_WORD)\n",
    "        print('Removed {} sentences'.format(len(sentences)-len(sentences_cleaned)))\n",
    "        df = pd.DataFrame(np.array([sentences_cleaned, [id_to_category[ll] for ll in labels]]).T, columns = ['utterance', 'category'])\n",
    "        df['category_id'] = labels\n",
    "    return df, features\n",
    "\n",
    "df, features = vectorize(df, vectorizer_name)\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze predictive features (works only for TFIDF)\n",
    "\n",
    "from sklearn.feature_selection import chi2\n",
    "import numpy as np\n",
    "N = 2\n",
    "features_chi2 = {}\n",
    "feature_names = {}\n",
    "tfidf = TfidfVectorizer(sublinear_tf=True, min_df=2, norm='l2', encoding='latin-1', ngram_range=(1, 2))\n",
    "features = tfidf.fit_transform(df.utterance).toarray()\n",
    "\n",
    "for Effekt, category_id in sorted(category_to_id.items()):\n",
    "  raw_chi2 = chi2(features, df.category_id == category_id)\n",
    "  indices = np.argsort(raw_chi2[0])[::-1]\n",
    "  feature_names[category_id] = np.array(tfidf.get_feature_names())[indices]\n",
    "  features_chi2[category_id] = raw_chi2[0][indices]\n",
    "  unigrams = [v for v in feature_names[category_id] if len(v.split(' ')) == 1]\n",
    "  bigrams = [v for v in feature_names[category_id] if len(v.split(' ')) == 2]\n",
    "  print(\"# '{}':\".format(Effekt))\n",
    "  print(\"  - Most correlated unigrams:\\n. {}\".format('\\n. '.join(unigrams[0:N])))\n",
    "  print(\"  - Most correlated bigrams:\\n. {}\".format('\\n. '.join(bigrams[0:N])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show feature predictiveness of features\n",
    "\n",
    "N = 20\n",
    "for index, cat in id_to_category.items():\n",
    "    fig = plt.figure(figsize=[20, 6])\n",
    "    plt.bar(range(N), features_chi2[index][0:N])\n",
    "    plt.xticks(range(N), feature_names[index][0:N])\n",
    "    plt.title(cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show MultinomialNB-result for specific utterance\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['utterance'], df['category'], random_state = 0)\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(X_train)\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "clf = MultinomialNB().fit(X_train_tfidf, y_train)\n",
    "print(clf.predict(count_vect.transform([\"Da musst du aber noch mal ran, das muss noch besser werden!\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare accuracies of different models\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "models = [\n",
    "    RandomForestClassifier(n_estimators=200, max_depth=3, random_state=0),\n",
    "    LinearSVC(),\n",
    "    SGDClassifier(tol=1e-3, max_iter=1000, loss='modified_huber'),\n",
    "    MultinomialNB(),\n",
    "    LogisticRegression(random_state=0),\n",
    "]\n",
    "\n",
    "CV = 5\n",
    "cv_df = pd.DataFrame(index=range(CV * len(models)))\n",
    "entries = []\n",
    "for model in models:\n",
    "  model_name = model.__class__.__name__\n",
    "  accuracies = cross_val_score(model, features, df['category_id'], scoring='accuracy', cv=CV)\n",
    "  for fold_idx, accuracy in enumerate(accuracies):\n",
    "    entries.append((model_name, fold_idx, accuracy))\n",
    "cv_df = pd.DataFrame(entries, columns=['model_name', 'fold_idx', 'accuracy'])\n",
    "import seaborn as sns\n",
    "sns.boxplot(x='model_name', y='accuracy', data=cv_df)\n",
    "sns.stripplot(x='model_name', y='accuracy', data=cv_df, \n",
    "              size=8, jitter=True, edgecolor=\"gray\", linewidth=2)\n",
    "plt.show()\n",
    "cv_df.groupby('model_name').accuracy.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## BUILD CONFUSION MATRIX WITH TRAIN/SPLIT\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "model = SGDClassifier(tol=1e-3, max_iter=1000, random_state=0, loss='modified_huber')\n",
    "# model = MultinomialNB()\n",
    "X_train, X_test, y_train, y_test, indices_train, indices_test = train_test_split(features, df.category_id, range(len(df.category_id)), test_size=0.33, random_state=0)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "conf_mat = confusion_matrix(y_test, y_pred)\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "sns.heatmap(conf_mat, annot=True, fmt='d',\n",
    "            xticklabels=category_id_df.category.values, yticklabels=category_id_df.category.values)\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SHOW WRONG SENTENCES WITH TRAIN/SPLIT\n",
    "\n",
    "from IPython.display import display\n",
    "import numpy as np\n",
    "\n",
    "for predicted in category_id_df.category_id:\n",
    "  for actual in category_id_df.category_id:\n",
    "    if predicted != actual and conf_mat[actual, predicted] >= 10:\n",
    "        selected_indices = y_test[(y_test == actual) & (y_pred == predicted)]\n",
    "        print(\"'{}' predicted as '{}' : {} examples.\".format(id_to_category[actual], id_to_category[predicted], conf_mat[actual, predicted]))\n",
    "        display(df.loc[selected_indices.index][['category', 'utterance']])\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## BUILD CONFUSION MATRIX WITH DEDICATED TEST-SET\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "# model = SGDClassifier(tol=1e-3, max_iter=1000, random_state=0, loss='modified_huber')\n",
    "model = MultinomialNB()\n",
    "\n",
    "df_test = trainer.load_training_files(data_path, 'TrainingDataTestSet01.tsv', categories_to_consider)\n",
    "# \n",
    "model.fit(features, df.category_id)\n",
    "\n",
    "if vectorizer_name == 'tfidf':\n",
    "    tfidf = TfidfVectorizer(sublinear_tf=True, min_df=2, norm='l2', encoding='latin-1', ngram_range=(1, 2))\n",
    "        # , stop_words=get_stop_words('de'))\n",
    "    tfidf.fit(df.utterance)\n",
    "    features_test = tfidf.transform(df_test.utterance)\n",
    "else:\n",
    "    df_test, features_test = vectorize(df_test, vectorizer_name)\n",
    "\n",
    "\n",
    "\n",
    "y_pred = model.predict(features_test)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "conf_mat = confusion_matrix(df_test.category_id, y_pred)\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "sns.heatmap(conf_mat, annot=True, fmt='d',\n",
    "            xticklabels=category_id_df.category.values, yticklabels=category_id_df.category.values)\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SHOW WRONG SENTENCES WITH DEDICATED TEST SET\n",
    "\n",
    "from IPython.display import display\n",
    "import numpy as np\n",
    "\n",
    "for predicted in category_id_df.category_id:\n",
    "  for actual in category_id_df.category_id:\n",
    "    if predicted != actual and conf_mat[actual, predicted] >= 1:\n",
    "        selected_indices = df_test.category_id[(df_test.category_id == actual) & (y_pred == predicted)]\n",
    "        print(\"'{}' predicted as '{}' : {} examples:\".format(id_to_category[actual], id_to_category[predicted], conf_mat[actual, predicted]))\n",
    "        for i, row in df_test.loc[selected_indices.index].iterrows():\n",
    "             print(row['utterance'])\n",
    "#         display(df_test.loc[selected_indices.index][['category', 'utterance']])\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check doc2vec\n",
    "# taken from https://medium.com/towards-artificial-intelligence/text-classification-by-xgboost-others-a-case-study-using-bbc-news-articles-5d88e94a9f8\n",
    "import pandas as pd\n",
    "\n",
    "file = '../webserver/model_data/TrainingData_ml.xlsx'\n",
    "df = pd.read_excel(file).rename(columns={'Effekt': 'category'})\n",
    "col = ['utterance', 'category']\n",
    "df = df[col]\n",
    "\n",
    "from gensim import utils\n",
    "import gensim.parsing.preprocessing as gsp\n",
    "\n",
    "filters = [\n",
    "           gsp.strip_tags, \n",
    "           gsp.strip_punctuation,\n",
    "           gsp.strip_multiple_whitespaces,\n",
    "           gsp.strip_numeric,\n",
    "           gsp.strip_short, \n",
    "          ]\n",
    "\n",
    "def clean_text(s):\n",
    "    s = s.lower()\n",
    "    s = utils.to_unicode(s)\n",
    "    for f in filters:\n",
    "        s = f(s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "\n",
    "def plot_word_cloud(text):\n",
    "    wordcloud_instance = WordCloud(width = 800, height = 800, \n",
    "                background_color ='black', \n",
    "                stopwords=None,\n",
    "                min_font_size = 10).generate(text) \n",
    "             \n",
    "    plt.figure(figsize = (8, 8), facecolor = None) \n",
    "    plt.imshow(wordcloud_instance) \n",
    "    plt.axis(\"off\") \n",
    "    plt.tight_layout(pad = 0) \n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "texts = ''\n",
    "for index, item in df.iterrows():\n",
    "    texts = texts + ' ' + clean_text(item['utterance'])\n",
    "    \n",
    "plot_word_cloud(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_word_cloud_for_category(df, category):\n",
    "    text_df = df.loc[df['category'] == str(category)]\n",
    "    texts = ''\n",
    "    for index, item in text_df.iterrows():\n",
    "        texts = texts + ' ' + clean_text(item['utterance'])\n",
    "    \n",
    "    plot_word_cloud(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###.  TEST GENSIM\n",
    "\n",
    "import sys, os\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "from webserver.classification import trainer\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "np.linalg.norm(trainer.sentence_to_vec_german_model('Berlin') - trainer.sentence_to_vec_german_model('Paris'))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
